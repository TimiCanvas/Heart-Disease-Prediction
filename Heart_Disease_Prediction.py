# -*- coding: utf-8 -*-
"""heart_disease_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q-O79Gx7klxmSkvmUy7TCYUmlsTizh6-
"""

# Commented out IPython magic to ensure Python compatibility.
# installing packages for analysis
import numpy as np
import pandas as pd
# %matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt

"""# Data Wrangling"""

# using pandas to read csv file
df = pd.read_csv('/content/sample_data/Train.csv')
# this returns the first 5 rows of the dataset to have an overview
df.head(5)

# gets quick info about the data
df.info()

# data shape
df.shape

# quick statistical description
df.describe()

# checking for missing values
df.isna().sum()

#checking for duplicates
df.duplicated().sum()

"""# Exploratory Data Analysis

- First, let's explore the relationship between all variables using the heatmap plot.
"""

#seaborn plot of the heatmap set to colour blue
plt.figure(figsize=(20,20))
sns.heatmap(df.corr(),cbar=True,annot=True,cmap='Blues')
plt.title('Heat map showing the correlation between all Variables');

"""- And a quick correlation analysis against the target variable"""

df.corr()['target'].sort_values(ascending=False)

"""The only variable showing a good postive correlation to the "target" is the "exang"."""

plt.figure(figsize=(15,7))
sns.boxplot(x = 'age', data = df)
plt.title('Box plot showing the summary statistics of city');

plt.figure(figsize=(15,7))
sns.boxplot(x = 'trestbps', data = df)
plt.title('Box plot showing the summary statistics of trestbps');

df['target'].value_counts().plot(kind='bar');

"""# Data Cleaning

- Data appears to be mostly clean.
- No missing values, nulls or duplicated rows.
- No outliers and all categorical datas are already recorded numerically.
- Data is also distributed normally, no need for normalization or standardization.

# Model Building

I will build a couple of models while compare their accuracy scores, before proceeding to use them for predictions on the test dataset.
"""

from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier

# Loading the train and test datasets
train_data = pd.read_csv('/content/sample_data/Train.csv')
test_data = pd.read_csv('/content/sample_data/Test.csv')

X = train_data.drop(columns=['target'])  # Features (all columns except 'target')
y = train_data['target']  # Target (the column I want to predict)

# Splitting the data into training and validation sets (80% training, 20% validation)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Random Forest Classifier"""

# Initializing the model
rf_model = RandomForestClassifier(random_state=42)

# Training the model on the training data
rf_model.fit(X_train, y_train)

# Making predictions on the validation set
y_pred_val = rf_model.predict(X_val)

# Evaluating the model's performance
print("RandomForestClassifier Validation Accuracy:", accuracy_score(y_val, y_pred_val))
print(classification_report(y_val, y_pred_val))

"""### Cross-Validating the RF model

Using the cross-validation method on the entire training set to figure if the accuracy score improves
"""

# Performing 5-fold cross-validation on the entire training dataset
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')

# Display the cross-validation scores and the average
print("Cross-Validation Scores:", cv_scores)
print("Average Cross-Validation Accuracy:", cv_scores.mean())

"""There is improvement.

### Tuned RF model

I also decided to tune the Random Forest Classifier model, using randomized search to better optimize the model.
"""

# Define the parameter grid
param_dist = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt'],
    'bootstrap': [True, False]
}

# Initialize the RandomForest model
rf = RandomForestClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,
                                   n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters:", random_search.best_params_)

# Get the best accuracy score
print("Best accuracy:", random_search.best_score_)

# Use the best model to make predictions
best_rf = random_search.best_estimator_
y_pred_rf = best_rf.predict(X_val)

# Calculate accuracy on the validation set
accuracy_rf_tuned = accuracy_score(y_val, y_pred_rf)
print("Tuned Random Forest Accuracy:", accuracy_rf_tuned)

# Using the best parameters found from RandomizedSearchCV
rf_tuned = RandomForestClassifier(
    n_estimators=500,
    max_depth=20,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    bootstrap=True,
    random_state=42
)


rf_tuned.fit(X_train, y_train)


y_pred_tuned_rf = rf_tuned.predict(X_val)


accuracy_rf_tuned = accuracy_score(y_val, y_pred_tuned_rf)
print("Tuned Random Forest Accuracy:", accuracy_rf_tuned)

"""The tuned model gives a higher accuracy than the original model."""

rf_tuned.fit(X, y)

# Making predictions on the test dataset
test_predictions = rf_tuned.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})

# Saving the submission file in a csv format
submission.to_csv('submission_rf_tuned.csv', index=False)

"""### RF Feature selection

Still in the bid of trying to improve performance, I decided to drop unwanted or unimportance features to see if there would be improvements.
"""

# Fitting the model and getting the important features
rf_tuned.fit(X_train, y_train)
importances = rf_tuned.feature_importances_
feature_names = X_train.columns

# Plotting feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_names, importances)
plt.xlabel("Feature Importance")
plt.title("Random Forest Feature Importance")
plt.show()

"""Based of this graph, I believe 8 features appears the most important. Therefore, using recursive feature elimination method, I would select the top 8 features, and retrain to see if there would be improvement in accuracy."""

# Initializing the RFE with Random Forest and select the top '8' features
rfe = RFE(estimator=rf_tuned, n_features_to_select=8)
rfe.fit(X_train, y_train)

# Getting the ranking of the features
rfe_ranking = rfe.ranking_

# Training a new model using only the selected features
X_train_rfe = X_train.iloc[:, rfe.support_]
X_val_rfe = X_val.iloc[:, rfe.support_]

rf_rfe = RandomForestClassifier(n_estimators=500, max_depth=20, random_state=42, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True)
rf_rfe.fit(X_train_rfe, y_train)

# Predicting and evaluate the performance on the validation set
y_pred_rfe = rf_rfe.predict(X_val_rfe)
rfe_accuracy = accuracy_score(y_val, y_pred_rfe)
print(f"Accuracy after Feature Selection: {rfe_accuracy}")

"""There is improvement, but not so significant."""

rf_rfe.fit(X, y)

test_predictions = rf_rfe.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})


submission.to_csv('submission_rf_rfe.csv', index=False)

"""## XGBoost

Checking if the XGBoost model gives a higher accuracy score than random forest.
"""

xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

y_pred_val = xgb_model.predict(X_val)


accuracy = accuracy_score(y_val, y_pred_val)
print(" XGBoost Validation Accuracy:", accuracy)
print(classification_report(y_val, y_pred_val))

"""The accuracy score is lower than that of the tuned RF model.

### Hyperparameter Tuning of the XGBoost model
"""

# Defining the parameter grid for tuning
param_grid = {
    'n_estimators': [100, 300, 500, 1000],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.3, 0.5]
}

# Setting up the RandomizedSearchCV for hyperparameter tuning
random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid,
                                   scoring='accuracy', cv=5, n_iter=50, verbose=1, random_state=42, n_jobs=-1)

# Fitting to the training data
random_search.fit(X_train, y_train)

# Getting the best parameters and the best score
print(f"Best parameters: {random_search.best_params_}")
print(f"Best accuracy: {random_search.best_score_}")

# Using the best parameters to fit the final model
xgb_new = random_search.best_estimator_
xgb_new.fit(X_train, y_train)

xgb_new.fit(X, y)

test_predictions = xgb_new.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})


submission.to_csv('submission_xgb_new.csv', index=False)

"""## Logistic Regression"""

# Initializing the Logistic Regression model
log_model = LogisticRegression(max_iter=1000, random_state=42)


log_model.fit(X_train, y_train)

y_pred_log = log_model.predict(X_val)


accuracy_log = accuracy_score(y_val, y_pred_log)
print("Logistic Regression Accuracy:", accuracy_log)

log_model.fit(X, y)

test_predictions = log_model.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})


submission.to_csv('submission_log.csv', index=False)

"""Although, the logistics model accuracy score is slightly higher than the tuned RF model, it does not do a better job at generalizing on unseen data unlike the tuned RF model.

## SVM
"""

# Initialize the SVM model
svm_model = SVC(random_state=42)

# Train the model
svm_model.fit(X_train, y_train)

# Predict on the validation set
y_pred_svm = svm_model.predict(X_val)

# Calculate accuracy
accuracy_svm = accuracy_score(y_val, y_pred_svm)
print("SVM Accuracy:", accuracy_svm)

"""Even with a higher accuracy score, the SVM model also faces the same issue with generalization on unseen data just like the logistic regression model.

### Tuned-SVM

# Defining parameter grid for SVM
param_grid = {'C': [0.01, 0.1, 1, 10, 100],
              'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
              'gamma': ['scale', 'auto'],
              'degree': [2, 3, 4]}

# Initialize SVM model
svm_model = SVC()

# Performing Grid Search
grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and accuracy
print("Best parameters:", grid_search.best_params_)
print("Best cross-validation accuracy:", grid_search.best_score_)

# Train SVM with best parameters and evaluate on the test set
best_svm = grid_search.best_estimator_
y_pred_svm = best_svm.predict(X_val)
accuracy_svm_tuned = accuracy_score(y_val, y_pred_svm)
print("Tuned SVM Accuracy:", accuracy_svm_tuned)"
"""

svm_model.fit(X, y)

test_predictions = svm_model.predict(test_data)

# Prepare the submission file
submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})

# Save the submission file
submission.to_csv('submission_svm.csv', index=False)

"""## KNN

Building the KNN model, to check for accuracy score also.
"""

knn_model = KNeighborsClassifier(n_neighbors=5)


knn_model.fit(X_train, y_train)


y_pred_knn = knn_model.predict(X_val)


accuracy_knn = accuracy_score(y_val, y_pred_knn)
print("KNN Accuracy:", accuracy_knn)

knn_model.fit(X, y)

test_predictions = knn_model.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})

# Save the submission file
submission.to_csv('submission_knn.csv', index=False)

"""The KNN accuracy score is not so great compared to previous models seen.

## Gradient Boosting
"""

gbc_model = GradientBoostingClassifier(random_state=42)


gbc_model.fit(X_train, y_train)


y_pred_gbc = gbc_model.predict(X_val)


accuracy_gbc = accuracy_score(y_val, y_pred_gbc)
print("Gradient Boosting Classifier Accuracy:", accuracy_gbc)

gbc_model.fit(X, y)

test_predictions = gbc_model.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})

# Save the submission file
submission.to_csv('submission_gbc.csv', index=False)

"""The Gradient Boosting model accuracy score is also not so great compared to other models seen before now.

## Model Stacking: RF Model, XGBoost and Logistic Regression

Since the RF model and XGBoost appears to do a good job at generalizing, I would explore options with them.

- The first option would be to perform model stacking which will combine the strength of the RF model and the XGBoost, using logistic regression as the meta-model to improve performance.
"""

# Base models
rf_rfe
xgb_model

# Stacking the base models with Logistic Regression as the meta-model
stacking_model = StackingClassifier(
    estimators=[
        ('rf', rf_rfe),
        ('xgb', xgb_model)
    ],
    final_estimator=LogisticRegression(),
    passthrough=False  # If True, features will be concatenated with predictions
)

# Train the stacked model
stacking_model.fit(X_train, y_train)

# Predict and evaluate
y_pred_stack = stacking_model.predict(X_val)
stacking_accuracy = accuracy_score(y_val, y_pred_stack)
print(f"Stacking Model (tuned Random Forest + XGBoost with LogisticRegression as meta-model) Accuracy: {stacking_accuracy}")

stacking_model.fit(X, y)

test_predictions = stacking_model.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})

submission.to_csv('submission_stacked_model.csv', index = False)

"""The stacked model's accuracy score appears to do better.

- The second option would be to make the meta-model in the stacked model, XGBoost instead of Logistic regression.
"""

#base models
rf_rfe
xgb_model

# Using XGBoost as the meta-model
stacking_model = StackingClassifier(
    estimators=[
        ('rf', rf_rfe),
        ('xgb', xgb_model)
    ],
    final_estimator=XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1),
    passthrough=False
)


stacking_model.fit(X_train, y_train)

# Predicting and evaluating
y_pred_stack = stacking_model.predict(X_val)
stacking_accuracy = accuracy_score(y_val, y_pred_stack)
print(f"Stacking Model (tuned Random Forest + XGBoost with XGBoost as meta-model) Accuracy: {stacking_accuracy}")

"""There is a significant improvement on the accuracy score with this new stacked model."""

stacking_model.fit(X, y)

stacking_model.predict(test_data)

submission = pd.DataFrame({
    'id': test_data['id'],
    'target': test_predictions
})

submission.to_csv('submission_stacked_model_new.csv', index = False)

"""## Model Choice

My model of choice is the stacked model, that is, the tuned Random Forest + XGBoost with LogisticRegression as the meta-model. It has an accuracy score of 0.8208744710860366 and does a great job at generalizing (performing well on unseen data).
"""